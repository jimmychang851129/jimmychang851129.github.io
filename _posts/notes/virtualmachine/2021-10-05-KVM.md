---
layout: post
title:  KVM
date:   2021-10-05 21:44:00
categories: VirtualMachine
tags: Course
---

### KVM (Kernel-based Virtual Machine)

- Type2 VMM(跑在OS上), 跟Linux kernel整合, 實現方式是一個Linux kernel module的形式(可以直接使用Linux的功能, 例如scheduler, Numa, IO, Interrupt管理,支援VM, Linux Process Management, 不用像Xen一樣還要一個Dom0來處理這些管控VM)
- 跟QEMU整合, KVM可分成兩個部份, 一個是跟qemu對接的接口, 另一個是跟底層architecture (x86, arm)對接的接口
- KVM為一種Linux character device(一種虛擬device介面, /dev/tty之類的那種, 讓user space如qemu application可以跟Linux kernel module溝通, user直接去那個路徑下對裡面file做操作就可以跟device互動)
- userspace可以跟/dev/kvm作互動來建立新VM, allocate memory, 讀寫virtual CPU暫存器, 跑vCPU之類的
- 運作方式如下圖, qemu跟VM跟linux裡的kvm module做溝通, kvm module再利用linux kernel內部的physical driver去存取hardware資源

![](/assets/images/notes/virtualmachine/3-3.png)

以下是KVM配合QEMU的運作模式, 其中左邊USER mode就是qemu的process, 中間kernel mode就是linux kernel, kvm的部份, 右邊就是VM execution, qemu會透過ioctl跟kvm溝通建立新的VM, 當guest VM需要handle IO時就會exit到KVM, KVM再根據需求跟qemu做溝通(因為device可能是由qemu模擬出來的), 所以Handle一個IO interrupt是非常花時間的(很多Context switch). 處理完interrupt後kvm會有虛擬的Interrupt Controller會再發virtual interrupt給VM, 然後while loop在重新enter guest.

![](/assets/images/notes/virtualmachine/3-4.png)

再跑VM時, QEMU會用mmap來allocate一塊看似連續的記憶體給Guest VM, 然後guest VM這塊paging則是由KVM來負責. 所以Guest VM的記憶體mapping是GPA map到qemu上的某個address(做mmap的關係), 再由qemu的那個address map到hpa

### ARM虛擬化

- Hyp mode會有不同的stack(el0,el1,el2)
- 每個stack都有自己的control register來設定一些方法來影響其他層(例如disabled trap)
- 跟x86_64不同的page table模式



### Trace code

#### qemu create VM

![](/assets/images/notes/virtualmachine/3-1.png)

#### create vcpu

![](/assets/images/notes/virtualmachine/3-2.png)