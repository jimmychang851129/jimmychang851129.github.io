---
layout: post
title:  "Memory Introduction"
date:   2019-05-09 21:44:00
categories: Computer-Architecture
tags: Course
---

## 記憶體

- ROM: Read Only,存取速度慢，但斷電資料還在，後來EPRROM是可以讀寫，但很麻煩，而且寫的速度慢
- RAM: 可讀寫，速度較快，但斷電資料消失
    - SRAM: 存取速度最快的RAM，但超貴
    - DRAM: 存取速度慢點的RAM，但比較便宜
    - SDRAM: 主要用於嵌入式
- Flash: 結合ROM,RAM特性，可以讀寫，斷電又不會遺失資料
    - Nor Flash: 跟sram依樣，能夠直接執行存在NorFlash上面的程式
    - Nand Flash:沒有random access，所以一次就是取一個大block(512 bytes)下來，適合大資料的存取，很適合那種一次要取多筆資料的應用

[RAM,ROM,Flash](https://www.itread01.com/content/1541799196.html)

### Principle of Locality

跑程式時，data的存取特性

- temporal locality: 現在被access到的data，不久未來有很高機率還會被access到(ex: for loop)
- Spatial locality: 被access到的data周圍data很有機率被access(ex: array)

因此抓資料時，不會只抓一點，一次會抓一個block來符合spatial locality，然後存在cache來符合temporal locality

### Memory hirearchy

![Imgur](https://i.imgur.com/rzlS7Vu.png)

#### 基本概念

因為記憶體要求讀取快速的話，價格非常昂貴，而讀取速度較慢的記憶體價格較便宜，但會有效能問題。<br />
為了達到最高CP值Memory會分層，越靠近CPU的記憶體價格越高，儲存空間小但快(如register / cache)，越遠離的則越便宜，儲存空間越大但慢(硬碟 / main memory)

### Main memory

word: 32 bits

- 資料平時都存在這，只有要用時才會把資料丟進register(register太小)。
- memory是byte-address(address為32-bits的字串)
- words在memory裡面是align

### Swap

基本上是disk，存取速度很慢，目的在於減緩memory,RAM的pressure，或者應付緊急狀況用。或者可以當成介於Memory跟disk中間的一層level cache。

當RAM的壓力過大(usage rate太高)，此時就會用到swap來舒緩RAM的壓力。通常用到swap時，程式速度會變超級慢。因此他可以某種程度的避免程式因為記憶體用盡所以crash，他能夠讓即使記憶體不足情況下還是能夠讓程式能執行一陣子，能夠給系統管理員更多發現問題的時間，或者給予程式更長執行時間，讓他能完成重要讀寫。

#### Reference

- [RedHat](https://www.redhat.com/en/blog/do-we-really-need-swap-modern-systems)
- [Ubuntu](https://help.ubuntu.com/community/SwapFaq#Why_do_I_need_swap.3F)
### Direct Mapped Cache

memory address名稱(共32bits)，每次access memory是搬一個block，block大小由OS定義

tag + index為一個block, offset表示block內有幾個word

index: 來決定要對應到哪個cache
tag: 可以說明該次的cache是由哪個tag的memory address提供的

Offset B.O: word裡面的第幾個byte(一個word 4個byte)
Offset W.O: word offset(cache裡面的第幾個word)

|  tag | index | offset(W.O,B.O) |
| -------- | -------- | -------- |


#### memory address naming

假設一個memory address有32bits，offset佔 5 bits(W.O 3 bits(代表一個block有8個words),B.O為2,一個word 2byte)，index佔3個bits(取決於cache大小)，剩下都是tag的bits

#### mechanism
Memory address對cache的對應是多對一，所以會有多個memory address對應到一個cache address，所以cache address只會放那幾個對應的memory address傳上來的值
所以假設很衰存取的幾個data的memory address剛好都是對應到一個cache address，那cache一直清掉重寫，一直cache miss

![Imgur](https://i.imgur.com/V2XSPKT.jpg)

#### Valid bits and tags(cache address naming)

由tags可以知道存在cache裡的資料來源是哪個memory address(tag + 該cache的index

Valid bits: 表示該cache現在裡面有沒有資料

![Imgur](https://i.imgur.com/2o7Jsp6.jpg)

#### Example 1

假設32-bit address(memory address的描述為32 - bit，cache可以用更少bit描述address)，directed map cache，

$$2^n$$ blocks<br />
$$2^m$$ block data size words

index = n bits
代表總共有m+2個bits的offset(m個words + 2個bits)

Tag size = 32 - (n+m+2)

Number of bits : block*(memory address佔用的bits + data佔用的bits)<br />
$$2^n (2^m*32+(32-n-m-2)+1))$$

#### Example 2

16KB data, 4-word blocks

1個word 4個byte，16KB data總共有4K個word

因此總共有1024個blocks(index)

cache size = block number * (memory address用bits + data)

cache address總共有18KB(原本data + cache memory address)

#### Example 3

![Imgur](https://i.imgur.com/l7FrDvE.jpg)

### Cache Block size consideration

block越大能存的data越多，越能發揮spatial locality，所以cache miss會減少。

缺點:
1. 但相對cache size固定，導致block number變少，變成更多對一，另一方面更容易cache miss，
2. block size太大會抓太多資料，很多可能都沒用到(pollution)，且cache miss的資料讀寫overhead變大(要讀寫更多byte, 因block size變大)


### Cache/Memory sync 機制

#### WriteThrough

保持cache,memory資料一致性，因為資料是從memory讀到cache，CPU取cache資料進行運算後，得到新的值寫回cache/register，這時不代表memory資料也update了。因此要有機制把cache資料寫回memory，確保memory的資料也有被更新。

把資料寫道cache時也一起寫到memory。<br />
耗時，寫cache時同時也要寫memory，會導致很多CPU cycle浪費

#### WriteBack

永遠只寫到cache，只有當block被踢出cache時(cache miss被overwrite)時，才會寫回memory(利用dirty bits來表示cache block有沒有被改過)


#### WriteBuffer

介於上述兩者之間，先把資料寫到cache / memory之間的buffer，從Buffer寫到memory寫完後才會把buffer清空。
慢的點為Buffer寫到memory，CPU平時不用等可以做自己的事，Buffer可以趁空擋寫到memory。只有當Buffer滿時，CPU才要halt, 等Buffer有空位才能把資料寫到Buffer再繼續進行。

### Write allocation

#### Write around

直接寫進memory，不特別讀到cache(某些資料他可能久久只存取一次，此時沒必要放到cache，直接對memory做讀寫,ex: init, memset)

#### Write-back

先讀到cache，從memory fetch block 

### Main memory to Cache

Memory(Dram)透過bus把資料送到cache，有clock bus，比CPU clocktime慢，所以傳送速度慢，

#### cache block read

把資料從Main memory需要的步驟及時間

1. 1 bus cycle for address transfer(傳address告訴main memory要讀哪塊資料)
2. 15 bus cycle for DRAM access(DRAM讀取資料1word所需時間)
3. 1 bus cycle per data transfer(1次傳1個word給cache縮需時間)

EX: 4-word block 1-word wide DRAM

Miss Pnealty = 1 + 15*4 + 4 * 1  = 65 bus cycle

所以一個Cache miss要把資料從memory寫到cache所需時間蠻多的

Bandwidth 16 / 65 ( Bytes/cycle)

其中2,3點可以做優化<br />
第2點:<br />
DRAM一次讀多個word時，可以把memory分成多個bank，不同word在不同的bank，那4個word讀取時可以同步，就不用sequential，花費時間=1個word，但還是要sequential透過bus傳給cache<br />
第3點: <br />
當然bus越多(4-word wide...)傳送速度會快更多

![Imgur](https://i.imgur.com/ma6A4e6.png)


### Cache performance measuring

Memory stall cycle(cache miss)<br />
= $$\frac{Memory Access}{Program}$$ * Miss rate * Miss penalty<br />

= $$\frac{instructions}{program} * \frac{Miss}{instructions}$$ * miss penalty

cache miss 有分data cache miss 或 instruction cache miss，data cache miss不一定會寫到cache，可能直接在memory操作

Memory stall一直是一個問題，即使現在CPU clock time越來越低，CPI越來越低，但是memory不便會拖慢速度，cache miss影響很顯著，消耗的clock cycle更多。

#### Average memory access time(AMAT)

AMAT = Hit time + Miss rate * Miss penalty

### Associative Cache

![Imgur](https://i.imgur.com/KjKrbLE.png)

#### n-way associative

- Memory寫到cache，他有複數個選擇，不是多對一，而是多對多，他可以選擇寫到數個cache blocks(看哪個是空的)
- 檢查n個entries就好
- 但是cache可能沒辦法100%的block都剛好用到，因為data只能被歸類在特定的幾個block，不是有空位就放


#### Fully associative

- 只要cache有位置，Memory就可以寫進去，Memory block跟cache block沒有對應的關係
- 每次找cache資料時，要每個entry都比對才知道資料在不在cache 
- cache能完整使用，100%使用，只要有空位就放，所以不會有某些index的data特別多導致miss rate很多

### Replacement Policy

從memory fetch一個新data，如果cache可以填的block滿了，就要把一個資料從cache剔除，才能把新的資料從memory寫進cache 

#### Directed Map 

沒選擇，因為是多對一，所以只有一個cache block，有cache miss(要其他tag的memory addr)，就只能把原有寫回memory，新的寫進cache

#### Leat-Recently used(LRU)

如果是associative cache，選最少用的(最久沒用的)寫回memory，空出空間給新的block

### Multi-level Cache

CPU -> Primary cache -> L2 cache -> L1 cache -> Main memory

越接近CPU越快、儲存空間越小，相反越慢、儲存空間越大

Primary cache: optimize hit time。<br />
L2 cache: 很怕miss，要optimize miss rate，因為一但miss，就要去memory，這部分的latency超大，所以要極力避免miss rate

miss rate根據memory arrangement pattern有關，和compiler, algorithm behavior之類的有關